{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T19:38:00.959602Z",
     "start_time": "2024-12-08T19:38:00.612121Z"
    }
   },
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "from bert import *\n",
    "from tf_idf import * \n",
    "from random_sample import *\n",
    "from w2v import *\n",
    "from ivec import *\n",
    "from vgg import *\n",
    "from resnet import *\n",
    "from incp import *\n",
    "from f_a import *\n",
    "from mfcc import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is properly set up\n",
    "print(torch.cuda.get_device_name(0))  # Should print your GPU's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos = pd.read_csv(\"dataset/id_information_mmsr.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 cores for TF-IDF processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing TF-IDF Recommendations: 100%|██████████| 5148/5148 [2:03:14<00:00,  1.44s/it]   \n"
     ]
    }
   ],
   "source": [
    "tf_idf = pd.read_csv(\"dataset/id_lyrics_tf-idf_mmsr.tsv\", sep=\"\\t\")\n",
    "recs_tf_idf = all_tf_idf_matrix(infos, tf_idf, topK=10)\n",
    "np.savetxt(\"./predictions/recs_tf_idf_10.csv\", recs_tf_idf, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 cores for BERT processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing BERT Recommendations:  13%|█▎        | 676/5148 [01:49<12:01,  6.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m bert \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/id_lyrics_bert_mmsr.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m recs_bert \u001b[38;5;241m=\u001b[39m \u001b[43mall_bert_recs_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./predictions/recs_bert_10.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, recs_bert, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hango\\OneDrive\\Dokumente\\2024WS\\MMSR\\multimedia-search-and-retrieval\\bert.py:103\u001b[0m, in \u001b[0;36mall_bert_recs_matrix\u001b[1;34m(infos, bert, topK)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm_joblib(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing BERT Recommendations\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(infos)):\n\u001b[1;32m--> 103\u001b[0m     recs \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_song\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43msong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msong\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minfos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(recs)\n",
      "File \u001b[1;32mc:\\Users\\hango\\OneDrive\\Dokumente\\2024WS\\MMSR\\multimedia-search-and-retrieval\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hango\\OneDrive\\Dokumente\\2024WS\\MMSR\\multimedia-search-and-retrieval\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hango\\OneDrive\\Dokumente\\2024WS\\MMSR\\multimedia-search-and-retrieval\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert = pd.read_csv(\"dataset/id_lyrics_bert_mmsr.tsv\", sep=\"\\t\")\n",
    "recs_bert = all_bert_recs_matrix(infos, bert, topK=10)\n",
    "np.savetxt(\"./predictions/recs_bert_10.csv\", recs_bert, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 cores for Random Recommendations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Random Recommendations: 100%|██████████| 5148/5148 [00:09<00:00, 542.21it/s]\n"
     ]
    }
   ],
   "source": [
    "recs_random = all_random_recs_matrix(infos, topK=10)\n",
    "np.savetxt(\"./predictions/recs_random_10.csv\", recs_random, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 cores for Word2Vec recommendation processing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07e59dbc4284e98bd9456c1dcec5634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Word2Vec Recommendations:   0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word2vec = pd.read_csv(\"dataset/id_lyrics_word2vec_mmsr.tsv\", sep=\"\\t\")  # Word2Vec embeddings\n",
    "recs_word2vec = all_word2vec_recs_matrix(infos, word2vec, topK=10)\n",
    "np.savetxt(\"./predictions/recs_word2vec_10.csv\", recs_word2vec, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 cores for iVector recommendation processing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6138fcff4d3344ce9c46334fa172b451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing iVector Recommendations:   0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ivec256 = pd.read_csv(\"dataset/id_ivec256_mmsr.tsv\", sep=\"\\t\")        # iVector (256 dimensions)\n",
    "ivec512 = pd.read_csv(\"dataset/id_ivec512_mmsr.tsv\", sep=\"\\t\")        # iVector (512 dimensions)\n",
    "ivec1024 = pd.read_csv(\"dataset/id_ivec1024_mmsr.tsv\", sep=\"\\t\")      # iVector (1024 dimensions)\n",
    "recs_ivec = all_ivec_recs_matrix(infos, ivec256, ivec512, ivec1024, topK=10)\n",
    "np.savetxt(\"./predictions/recs_ivec_10.csv\", recs_ivec, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 cores for VGG19 recommendation processing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75c1d4071be4d0bb7cfa44f987ef3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing VGG19 Recommendations:   0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vgg19 = pd.read_csv(\"dataset/id_vgg19_mmsr.tsv\", sep=\"\\t\")        # VGG19 features\n",
    "recs_vgg19 = all_vgg19_recs_matrix(infos, vgg19, topK=10)\n",
    "np.savetxt(\"./predictions/recs_vgg19_10.csv\", recs_vgg19, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 cores for ResNet processing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fad9f5c47aa4f54807d7a99c91cd9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing ResNet Recommendations:   0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet = pd.read_csv(\"dataset/id_resnet_mmsr.tsv\", sep=\"\\t\")\n",
    "recs_resnet = all_resnet_recs_matrix(infos, resnet, topK=10)\n",
    "np.savetxt(\"./predictions/recs_resnet_10.csv\", recs_resnet, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 cores for Inception processing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a053765fee4f91ae770d32f20055c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Inception Recommendations:   0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inception = pd.read_csv(\"dataset/id_incp_mmsr.tsv\", sep=\"\\t\")\n",
    "recs_inception = all_inception_recs_matrix(infos, inception, topK=10)\n",
    "np.savetxt(\"./predictions/recs_incp_10.csv\", recs_inception, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 cores for feature-aware recommendation generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acdc05aa36c4c12a193b439fc32e055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Feature-Aware Recommendations:   0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "genres = pd.read_csv(\"dataset/id_genres_mmsr.tsv\", sep=\"\\t\")\n",
    "tags = pd.read_csv(\"dataset/id_tags_dict.tsv\", sep=\"\\t\")\n",
    "ivec256 = pd.read_csv(\"dataset/id_ivec256_mmsr.tsv\", sep=\"\\t\")\n",
    "ivec512 = pd.read_csv(\"dataset/id_ivec512_mmsr.tsv\", sep=\"\\t\")\n",
    "ivec1024 = pd.read_csv(\"dataset/id_ivec1024_mmsr.tsv\", sep=\"\\t\")\n",
    "recs_fa = all_feature_aware_recs(\n",
    "    infos=infos,\n",
    "    genres=genres,\n",
    "    tags=tags,\n",
    "    ivec256=ivec256,\n",
    "    ivec512=ivec512,\n",
    "    ivec1024=ivec1024,\n",
    "    topK=10,        # Number of recommendations\n",
    "    alpha=1.0,      # Weight for genres\n",
    "    beta=1.0,       # Weight for tags\n",
    "    gamma=2.0       # Weight for ivec\n",
    ")\n",
    "np.savetxt(\"./predictions/recs_f_a_10.csv\", recs_fa, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 cores for MFCC processing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931520df8c1d40a6a6898817b1e109c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing BERT Recommendations:   0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mfcc = pd.read_csv(\"dataset/id_mfcc_stats_mmsr.tsv\", sep=\"\\t\")\n",
    "recs_mfcc = all_mfcc_recs_matrix(infos, mfcc, topK=10)\n",
    "np.savetxt(\"./predictions/recs_mfcc_stats_10.csv\", recs_mfcc, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 cores for MFCC processing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110bb57a835a4522b011263a234efa6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing BERT Recommendations:   0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mfcc = pd.read_csv(\"dataset/id_mfcc_bow_mmsr.tsv\", sep=\"\\t\")\n",
    "recs_mfcc = all_mfcc_recs_matrix(infos, mfcc, topK=10)\n",
    "np.savetxt(\"./predictions/recs_mfcc_bow_10.csv\", recs_mfcc, delimiter=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
