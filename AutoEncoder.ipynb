{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51de3d-be82-4428-8528-894b8141b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b, eps=1e-9):\n",
    "    dot = np.dot(vec_a, vec_b)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    return dot / ((norm_a * norm_b) + eps)\n",
    "\n",
    "\n",
    "def load_and_merge_numeric_features(file_list, merge_on='id'):\n",
    "    base_df = pd.read_csv(file_list[0], sep='\\t')\n",
    "    for path in file_list[1:]:\n",
    "        df_next = pd.read_csv(path, sep='\\t')\n",
    "        base_df = pd.merge(base_df, df_next, on=merge_on, how='inner')\n",
    "    base_df.set_index(merge_on, inplace=True)\n",
    "    return base_df\n",
    "\n",
    "def normalize_features(df):\n",
    "    scaler = StandardScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
    "\n",
    "class Music4AllOnionDataset(Dataset):\n",
    "    def __init__(self, feature_df, label_df=None, transform=None):\n",
    "        self.transform = transform\n",
    "        feature_df = feature_df.sort_index()\n",
    "        self.feature_df = feature_df\n",
    "\n",
    "        if label_df is not None:\n",
    "            label_df = label_df.sort_index()\n",
    "            common_idx = feature_df.index.intersection(label_df.index)\n",
    "            self.feature_df = feature_df.loc[common_idx]\n",
    "            self.label_df = label_df.loc[common_idx]\n",
    "            self.has_labels = True\n",
    "        else:\n",
    "            self.label_df = None\n",
    "            self.has_labels = False\n",
    "\n",
    "        self.feature_data = self.feature_df.values.astype(np.float32)\n",
    "        if self.transform:\n",
    "            self.feature_data = self.transform(self.feature_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.feature_data[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        if self.has_labels:\n",
    "            y_val = self.label_df.iloc[idx].values\n",
    "            y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "            return x, y_val\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=128):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, reconstruct=False):\n",
    "        z = self.encoder(x)\n",
    "        if reconstruct:\n",
    "            return self.decoder(z)\n",
    "        return z\n",
    "\n",
    "class FineTuneClassifier(nn.Module):\n",
    "    def __init__(self, autoenc, latent_dim, num_classes):\n",
    "        super(FineTuneClassifier, self).__init__()\n",
    "        self.autoenc = autoenc\n",
    "        for param in self.autoenc.encoder.parameters():\n",
    "            param.requires_grad = False  \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            z = self.autoenc.encoder(x)  \n",
    "        logits = self.classifier(z)\n",
    "        return logits\n",
    "\n",
    "def train_autoenc(model, dataloader, num_epochs=50, lr=1e-3, patience=5, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon = model(x, reconstruct=True)\n",
    "            loss = criterion(x_recon, x)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "def train_classifier(\n",
    "    model, dataloader, num_epochs=20, lr=1e-3, patience=5, device='cuda'\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y.squeeze().long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"[Classifier Epoch {epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "def compute_all_embeddings(dataset, model, device='cuda'):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        item = dataset[i]\n",
    "        x = item[0] if isinstance(item, tuple) else item\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            z = model(x)\n",
    "        z_np = z.cpu().numpy().flatten()\n",
    "        all_embeddings.append(z_np)\n",
    "\n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def build_recommendation_matrix(embeddings, topK=10):\n",
    "    N = len(embeddings)\n",
    "    rec_matrix = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "    for i in tqdm(range(N), desc=\"Building Recs\"):\n",
    "        vec_i = embeddings[i]\n",
    "        sims = [cosine_similarity(vec_i, embeddings[j]) for j in range(N)]\n",
    "        sims = np.array(sims)\n",
    "        sims[i] = -1e9\n",
    "        topk_idx = sims.argsort()[::-1][:topK]\n",
    "        topk_vals = sims[topk_idx]\n",
    "        rec_matrix[i, topk_idx] = topk_vals\n",
    "\n",
    "    return rec_matrix\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "numeric_files = [\n",
    "        \"id_blf_correlation_mmsr.tsv\",\n",
    "        \"id_blf_deltaspectral_mmsr.tsv\",\n",
    "        \"id_blf_logfluc_mmsr.tsv\",\n",
    "        \"id_blf_spectral_mmsr.tsv\",\n",
    "        \"id_blf_spectralcontrast_mmsr.tsv\",\n",
    "        \"id_blf_vardeltaspectral_mmsr.tsv\",\n",
    "        \"id_incp_mmsr.tsv\",\n",
    "        \"id_ivec256_mmsr.tsv\",\n",
    "        \"id_ivec512_mmsr.tsv\",\n",
    "        \"id_ivec1024_mmsr.tsv\",\n",
    "        \"id_lyrics_tf-idf_mmsr.tsv\",\n",
    "        \"id_lyrics_word2vec_mmsr.tsv\",\n",
    "        \"id_lyrics_bert_mmsr.tsv\",\n",
    "        \"id_mfcc_bow_mmsr.tsv\",\n",
    "        \"id_mfcc_stats_mmsr.tsv\",\n",
    "        \"id_musicnn_mmsr.tsv\",\n",
    "        \"id_resnet_mmsr.tsv\",\n",
    "        \"id_vgg19_mmsr.tsv\",\n",
    "        \"id_total_listens.tsv\"\n",
    "]\n",
    "\n",
    "fused_df = load_and_merge_numeric_features(numeric_files, merge_on='id')\n",
    "fused_df_normalized = normalize_features(fused_df)\n",
    "\n",
    "labels_path = \"id_genres_mmsr.tsv\"\n",
    "if os.path.exists(labels_path):\n",
    "        labels_df = pd.read_csv(labels_path, sep='\\t').set_index('id')\n",
    "        unique_genres = labels_df['genre'].unique()\n",
    "        genre_to_idx = {g: i for i, g in enumerate(unique_genres)}\n",
    "        labels_df['genre_id'] = labels_df['genre'].map(genre_to_idx)\n",
    "        final_labels_df = labels_df[['genre_id']]\n",
    "else:\n",
    "    final_labels_df = None\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(\n",
    "        fused_df_normalized, final_labels_df, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Music4AllOnionDataset(feature_train, label_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "test_dataset = Music4AllOnionDataset(feature_test, label_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "input_dim = fused_df_normalized.shape[1]\n",
    "latent_dim = 128\n",
    "\n",
    "autoenc_model = AutoEncoder(input_dim, latent_dim=latent_dim)\n",
    "train_autoenc(autoenc_model, train_loader, num_epochs=250, lr=1e-3, patience=100, device=device)\n",
    "\n",
    "if final_labels_df is not None:\n",
    "    num_classes = len(np.unique(final_labels_df['genre_id']))\n",
    "    classifier_model = FineTuneClassifier(mkgcn_model, latent_dim, num_classes)\n",
    "    train_classifier(classifier_model, train_loader, num_epochs=250, patience=100, lr=1e-3, device=device)\n",
    "\n",
    "full_dataset = Music4AllOnionDataset(fused_df_normalized)\n",
    "embeddings = compute_all_embeddings(full_dataset, mkgcn_model, device=device)\n",
    "rec_matrix = build_recommendation_matrix(embeddings, topK=10)\n",
    "np.savetxt(\"recs_autoenc_1_10.csv\", rec_matrix, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbad02-a9c7-4572-8556-bab283e4b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_recommendations(embeddings, ids, infos, topK=10):\n",
    "    recommendations = []\n",
    "    N = len(embeddings)\n",
    "\n",
    "    for i in tqdm(range(N), desc=\"Building Recommendations\"):\n",
    "        vec_i = embeddings[i]\n",
    "        sims = [\n",
    "            {\"source_id\": ids[i], \"target_id\": ids[j], \"similarity\": cosine_similarity(vec_i, embeddings[j])}\n",
    "            for j in range(N) if i != j\n",
    "        ]\n",
    "        top_k_recs = sorted(sims, key=lambda x: x[\"similarity\"], reverse=True)[:topK]\n",
    "        recommendations.extend(top_k_recs)\n",
    "\n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "full_dataset = Music4AllOnionDataset(fused_df_normalized)\n",
    "embeddings = compute_all_embeddings(full_dataset, mkgcn_model, device=device)\n",
    "ids = fused_df_normalized.index.tolist()\n",
    "recommendations = build_recommendations(embeddings, ids, fused_df_normalized, topK=100)\n",
    "recommendations.to_csv(\"recs_autoenc_10.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
