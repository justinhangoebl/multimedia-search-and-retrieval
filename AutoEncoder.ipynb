{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d51de3d-be82-4428-8528-894b8141b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/250] Loss: 0.8646\n",
      "[Epoch 2/250] Loss: 0.7881\n",
      "[Epoch 3/250] Loss: 0.7669\n",
      "[Epoch 4/250] Loss: 0.7520\n",
      "[Epoch 5/250] Loss: 0.7438\n",
      "[Epoch 6/250] Loss: 0.7350\n",
      "[Epoch 7/250] Loss: 0.7278\n",
      "[Epoch 8/250] Loss: 0.7255\n",
      "[Epoch 9/250] Loss: 0.7214\n",
      "[Epoch 10/250] Loss: 0.7164\n",
      "[Epoch 11/250] Loss: 0.7123\n",
      "[Epoch 12/250] Loss: 0.7082\n",
      "[Epoch 13/250] Loss: 0.7038\n",
      "[Epoch 14/250] Loss: 0.7031\n",
      "[Epoch 15/250] Loss: 0.6979\n",
      "[Epoch 16/250] Loss: 0.6968\n",
      "[Epoch 17/250] Loss: 0.6935\n",
      "[Epoch 18/250] Loss: 0.6915\n",
      "[Epoch 19/250] Loss: 0.6882\n",
      "[Epoch 20/250] Loss: 0.6855\n",
      "[Epoch 21/250] Loss: 0.6860\n",
      "[Epoch 22/250] Loss: 0.6845\n",
      "[Epoch 23/250] Loss: 0.6810\n",
      "[Epoch 24/250] Loss: 0.6787\n",
      "[Epoch 25/250] Loss: 0.6764\n",
      "[Epoch 26/250] Loss: 0.6760\n",
      "[Epoch 27/250] Loss: 0.6741\n",
      "[Epoch 28/250] Loss: 0.6740\n",
      "[Epoch 29/250] Loss: 0.6727\n",
      "[Epoch 30/250] Loss: 0.6705\n",
      "[Epoch 31/250] Loss: 0.6691\n",
      "[Epoch 32/250] Loss: 0.6699\n",
      "[Epoch 33/250] Loss: 0.6676\n",
      "[Epoch 34/250] Loss: 0.6655\n",
      "[Epoch 35/250] Loss: 0.6673\n",
      "[Epoch 36/250] Loss: 0.6647\n",
      "[Epoch 37/250] Loss: 0.6629\n",
      "[Epoch 38/250] Loss: 0.6639\n",
      "[Epoch 39/250] Loss: 0.6623\n",
      "[Epoch 40/250] Loss: 0.6608\n",
      "[Epoch 41/250] Loss: 0.6622\n",
      "[Epoch 42/250] Loss: 0.6624\n",
      "[Epoch 43/250] Loss: 0.6592\n",
      "[Epoch 44/250] Loss: 0.6591\n",
      "[Epoch 45/250] Loss: 0.6581\n",
      "[Epoch 46/250] Loss: 0.6582\n",
      "[Epoch 47/250] Loss: 0.6596\n",
      "[Epoch 48/250] Loss: 0.6566\n",
      "[Epoch 49/250] Loss: 0.6557\n",
      "[Epoch 50/250] Loss: 0.6558\n",
      "[Epoch 51/250] Loss: 0.6559\n",
      "[Epoch 52/250] Loss: 0.6527\n",
      "[Epoch 53/250] Loss: 0.6561\n",
      "[Epoch 54/250] Loss: 0.6542\n",
      "[Epoch 55/250] Loss: 0.6536\n",
      "[Epoch 56/250] Loss: 0.6539\n",
      "[Epoch 57/250] Loss: 0.6471\n",
      "[Epoch 58/250] Loss: 0.6468\n",
      "[Epoch 59/250] Loss: 0.6459\n",
      "[Epoch 60/250] Loss: 0.6448\n",
      "[Epoch 61/250] Loss: 0.6458\n",
      "[Epoch 62/250] Loss: 0.6443\n",
      "[Epoch 63/250] Loss: 0.6428\n",
      "[Epoch 64/250] Loss: 0.6440\n",
      "[Epoch 65/250] Loss: 0.6433\n",
      "[Epoch 66/250] Loss: 0.6437\n",
      "[Epoch 67/250] Loss: 0.6426\n",
      "[Epoch 68/250] Loss: 0.6425\n",
      "[Epoch 69/250] Loss: 0.6430\n",
      "[Epoch 70/250] Loss: 0.6421\n",
      "[Epoch 71/250] Loss: 0.6428\n",
      "[Epoch 72/250] Loss: 0.6433\n",
      "[Epoch 73/250] Loss: 0.6435\n",
      "[Epoch 74/250] Loss: 0.6403\n",
      "[Epoch 75/250] Loss: 0.6405\n",
      "[Epoch 76/250] Loss: 0.6417\n",
      "[Epoch 77/250] Loss: 0.6415\n",
      "[Epoch 78/250] Loss: 0.6410\n",
      "[Epoch 79/250] Loss: 0.6391\n",
      "[Epoch 80/250] Loss: 0.6410\n",
      "[Epoch 81/250] Loss: 0.6402\n",
      "[Epoch 82/250] Loss: 0.6396\n",
      "[Epoch 83/250] Loss: 0.6412\n",
      "[Epoch 84/250] Loss: 0.6414\n",
      "[Epoch 85/250] Loss: 0.6418\n",
      "[Epoch 86/250] Loss: 0.6406\n",
      "[Epoch 87/250] Loss: 0.6424\n",
      "[Epoch 88/250] Loss: 0.6394\n",
      "[Epoch 89/250] Loss: 0.6406\n",
      "[Epoch 90/250] Loss: 0.6411\n",
      "[Epoch 91/250] Loss: 0.6413\n",
      "[Epoch 92/250] Loss: 0.6409\n",
      "[Epoch 93/250] Loss: 0.6413\n",
      "[Epoch 94/250] Loss: 0.6408\n",
      "[Epoch 95/250] Loss: 0.6388\n",
      "[Epoch 96/250] Loss: 0.6417\n",
      "[Epoch 97/250] Loss: 0.6398\n",
      "[Epoch 98/250] Loss: 0.6393\n",
      "[Epoch 99/250] Loss: 0.6386\n",
      "[Epoch 100/250] Loss: 0.6399\n",
      "[Epoch 101/250] Loss: 0.6406\n",
      "[Epoch 102/250] Loss: 0.6408\n",
      "[Epoch 103/250] Loss: 0.6408\n",
      "[Epoch 104/250] Loss: 0.6397\n",
      "[Epoch 105/250] Loss: 0.6391\n",
      "[Epoch 106/250] Loss: 0.6406\n",
      "[Epoch 107/250] Loss: 0.6401\n",
      "[Epoch 108/250] Loss: 0.6405\n",
      "[Epoch 109/250] Loss: 0.6400\n",
      "[Epoch 110/250] Loss: 0.6408\n",
      "[Epoch 111/250] Loss: 0.6394\n",
      "[Epoch 112/250] Loss: 0.6408\n",
      "[Epoch 113/250] Loss: 0.6403\n",
      "[Epoch 114/250] Loss: 0.6399\n",
      "[Epoch 115/250] Loss: 0.6402\n",
      "[Epoch 116/250] Loss: 0.6378\n",
      "[Epoch 117/250] Loss: 0.6404\n",
      "[Epoch 118/250] Loss: 0.6397\n",
      "[Epoch 119/250] Loss: 0.6397\n",
      "[Epoch 120/250] Loss: 0.6408\n",
      "[Epoch 121/250] Loss: 0.6400\n",
      "[Epoch 122/250] Loss: 0.6417\n",
      "[Epoch 123/250] Loss: 0.6410\n",
      "[Epoch 124/250] Loss: 0.6414\n",
      "[Epoch 125/250] Loss: 0.6405\n",
      "[Epoch 126/250] Loss: 0.6406\n",
      "[Epoch 127/250] Loss: 0.6400\n",
      "[Epoch 128/250] Loss: 0.6419\n",
      "[Epoch 129/250] Loss: 0.6381\n",
      "[Epoch 130/250] Loss: 0.6411\n",
      "[Epoch 131/250] Loss: 0.6400\n",
      "[Epoch 132/250] Loss: 0.6412\n",
      "[Epoch 133/250] Loss: 0.6394\n",
      "[Epoch 134/250] Loss: 0.6411\n",
      "[Epoch 135/250] Loss: 0.6396\n",
      "[Epoch 136/250] Loss: 0.6404\n",
      "[Epoch 137/250] Loss: 0.6386\n",
      "[Epoch 138/250] Loss: 0.6396\n",
      "[Epoch 139/250] Loss: 0.6405\n",
      "[Epoch 140/250] Loss: 0.6403\n",
      "[Epoch 141/250] Loss: 0.6394\n",
      "[Epoch 142/250] Loss: 0.6402\n",
      "[Epoch 143/250] Loss: 0.6405\n",
      "[Epoch 144/250] Loss: 0.6405\n",
      "[Epoch 145/250] Loss: 0.6391\n",
      "[Epoch 146/250] Loss: 0.6411\n",
      "[Epoch 147/250] Loss: 0.6415\n",
      "[Epoch 148/250] Loss: 0.6404\n",
      "[Epoch 149/250] Loss: 0.6393\n",
      "[Epoch 150/250] Loss: 0.6401\n",
      "[Epoch 151/250] Loss: 0.6397\n",
      "[Epoch 152/250] Loss: 0.6398\n",
      "[Epoch 153/250] Loss: 0.6404\n",
      "[Epoch 154/250] Loss: 0.6386\n",
      "[Epoch 155/250] Loss: 0.6408\n",
      "[Epoch 156/250] Loss: 0.6391\n",
      "[Epoch 157/250] Loss: 0.6397\n",
      "[Epoch 158/250] Loss: 0.6411\n",
      "[Epoch 159/250] Loss: 0.6407\n",
      "[Epoch 160/250] Loss: 0.6398\n",
      "[Epoch 161/250] Loss: 0.6407\n",
      "[Epoch 162/250] Loss: 0.6408\n",
      "[Epoch 163/250] Loss: 0.6393\n",
      "[Epoch 164/250] Loss: 0.6406\n",
      "[Epoch 165/250] Loss: 0.6405\n",
      "[Epoch 166/250] Loss: 0.6405\n",
      "[Epoch 167/250] Loss: 0.6410\n",
      "[Epoch 168/250] Loss: 0.6399\n",
      "[Epoch 169/250] Loss: 0.6396\n",
      "[Epoch 170/250] Loss: 0.6403\n",
      "[Epoch 171/250] Loss: 0.6407\n",
      "[Epoch 172/250] Loss: 0.6408\n",
      "[Epoch 173/250] Loss: 0.6403\n",
      "[Epoch 174/250] Loss: 0.6405\n",
      "[Epoch 175/250] Loss: 0.6396\n",
      "[Epoch 176/250] Loss: 0.6403\n",
      "[Epoch 177/250] Loss: 0.6406\n",
      "[Epoch 178/250] Loss: 0.6409\n",
      "[Epoch 179/250] Loss: 0.6408\n",
      "[Epoch 180/250] Loss: 0.6400\n",
      "[Epoch 181/250] Loss: 0.6391\n",
      "[Epoch 182/250] Loss: 0.6395\n",
      "[Epoch 183/250] Loss: 0.6389\n",
      "[Epoch 184/250] Loss: 0.6409\n",
      "[Epoch 185/250] Loss: 0.6384\n",
      "[Epoch 186/250] Loss: 0.6410\n",
      "[Epoch 187/250] Loss: 0.6414\n",
      "[Epoch 188/250] Loss: 0.6398\n",
      "[Epoch 189/250] Loss: 0.6425\n",
      "[Epoch 190/250] Loss: 0.6386\n",
      "[Epoch 191/250] Loss: 0.6386\n",
      "[Epoch 192/250] Loss: 0.6398\n",
      "[Epoch 193/250] Loss: 0.6415\n",
      "[Epoch 194/250] Loss: 0.6402\n",
      "[Epoch 195/250] Loss: 0.6393\n",
      "[Epoch 196/250] Loss: 0.6398\n",
      "[Epoch 197/250] Loss: 0.6405\n",
      "[Epoch 198/250] Loss: 0.6400\n",
      "[Epoch 199/250] Loss: 0.6406\n",
      "[Epoch 200/250] Loss: 0.6402\n",
      "[Epoch 201/250] Loss: 0.6403\n",
      "[Epoch 202/250] Loss: 0.6403\n",
      "[Epoch 203/250] Loss: 0.6406\n",
      "[Epoch 204/250] Loss: 0.6392\n",
      "[Epoch 205/250] Loss: 0.6407\n",
      "[Epoch 206/250] Loss: 0.6398\n",
      "[Epoch 207/250] Loss: 0.6397\n",
      "[Epoch 208/250] Loss: 0.6395\n",
      "[Epoch 209/250] Loss: 0.6417\n",
      "[Epoch 210/250] Loss: 0.6403\n",
      "[Epoch 211/250] Loss: 0.6397\n",
      "[Epoch 212/250] Loss: 0.6388\n",
      "[Epoch 213/250] Loss: 0.6400\n",
      "[Epoch 214/250] Loss: 0.6392\n",
      "[Epoch 215/250] Loss: 0.6391\n",
      "[Epoch 216/250] Loss: 0.6400\n",
      "Early stopping triggered at epoch 216\n",
      "[Classifier Epoch 1/250] Loss: 8.4446\n",
      "[Classifier Epoch 2/250] Loss: 7.7131\n",
      "[Classifier Epoch 3/250] Loss: 6.4531\n",
      "[Classifier Epoch 4/250] Loss: 4.9674\n",
      "[Classifier Epoch 5/250] Loss: 3.5927\n",
      "[Classifier Epoch 6/250] Loss: 2.5582\n",
      "[Classifier Epoch 7/250] Loss: 1.9732\n",
      "[Classifier Epoch 8/250] Loss: 1.7073\n",
      "[Classifier Epoch 9/250] Loss: 1.4929\n",
      "[Classifier Epoch 10/250] Loss: 1.3449\n",
      "[Classifier Epoch 11/250] Loss: 1.2249\n",
      "[Classifier Epoch 12/250] Loss: 1.1495\n",
      "[Classifier Epoch 13/250] Loss: 1.1276\n",
      "[Classifier Epoch 14/250] Loss: 1.0225\n",
      "[Classifier Epoch 15/250] Loss: 1.0188\n",
      "[Classifier Epoch 16/250] Loss: 0.9723\n",
      "[Classifier Epoch 17/250] Loss: 0.9119\n",
      "[Classifier Epoch 18/250] Loss: 0.8788\n",
      "[Classifier Epoch 19/250] Loss: 0.8274\n",
      "[Classifier Epoch 20/250] Loss: 0.8284\n",
      "[Classifier Epoch 21/250] Loss: 0.7745\n",
      "[Classifier Epoch 22/250] Loss: 0.7836\n",
      "[Classifier Epoch 23/250] Loss: 0.7885\n",
      "[Classifier Epoch 24/250] Loss: 0.7390\n",
      "[Classifier Epoch 25/250] Loss: 0.7754\n",
      "[Classifier Epoch 26/250] Loss: 0.7173\n",
      "[Classifier Epoch 27/250] Loss: 0.7252\n",
      "[Classifier Epoch 28/250] Loss: 0.7090\n",
      "[Classifier Epoch 29/250] Loss: 0.6634\n",
      "[Classifier Epoch 30/250] Loss: 0.6615\n",
      "[Classifier Epoch 31/250] Loss: 0.6422\n",
      "[Classifier Epoch 32/250] Loss: 0.6576\n",
      "[Classifier Epoch 33/250] Loss: 0.6304\n",
      "[Classifier Epoch 34/250] Loss: 0.6049\n",
      "[Classifier Epoch 35/250] Loss: 0.6092\n",
      "[Classifier Epoch 36/250] Loss: 0.6020\n",
      "[Classifier Epoch 37/250] Loss: 0.6131\n",
      "[Classifier Epoch 38/250] Loss: 0.6256\n",
      "[Classifier Epoch 39/250] Loss: 0.6078\n",
      "[Classifier Epoch 40/250] Loss: 0.5749\n",
      "[Classifier Epoch 41/250] Loss: 0.6205\n",
      "[Classifier Epoch 42/250] Loss: 0.5603\n",
      "[Classifier Epoch 43/250] Loss: 0.5380\n",
      "[Classifier Epoch 44/250] Loss: 0.5434\n",
      "[Classifier Epoch 45/250] Loss: 0.5061\n",
      "[Classifier Epoch 46/250] Loss: 0.5420\n",
      "[Classifier Epoch 47/250] Loss: 0.5374\n",
      "[Classifier Epoch 48/250] Loss: 0.5668\n",
      "[Classifier Epoch 49/250] Loss: 0.5282\n",
      "[Classifier Epoch 50/250] Loss: 0.5299\n",
      "[Classifier Epoch 51/250] Loss: 0.5455\n",
      "[Classifier Epoch 52/250] Loss: 0.5128\n",
      "[Classifier Epoch 53/250] Loss: 0.5148\n",
      "[Classifier Epoch 54/250] Loss: 0.5261\n",
      "[Classifier Epoch 55/250] Loss: 0.5466\n",
      "[Classifier Epoch 56/250] Loss: 0.5260\n",
      "[Classifier Epoch 57/250] Loss: 0.5138\n",
      "[Classifier Epoch 58/250] Loss: 0.5387\n",
      "[Classifier Epoch 59/250] Loss: 0.4951\n",
      "[Classifier Epoch 60/250] Loss: 0.5248\n",
      "[Classifier Epoch 61/250] Loss: 0.4975\n",
      "[Classifier Epoch 62/250] Loss: 0.5003\n",
      "[Classifier Epoch 63/250] Loss: 0.4946\n",
      "[Classifier Epoch 64/250] Loss: 0.5101\n",
      "[Classifier Epoch 65/250] Loss: 0.4712\n",
      "[Classifier Epoch 66/250] Loss: 0.5238\n",
      "[Classifier Epoch 67/250] Loss: 0.4840\n",
      "[Classifier Epoch 68/250] Loss: 0.4883\n",
      "[Classifier Epoch 69/250] Loss: 0.4837\n",
      "[Classifier Epoch 70/250] Loss: 0.4706\n",
      "[Classifier Epoch 71/250] Loss: 0.4868\n",
      "[Classifier Epoch 72/250] Loss: 0.4531\n",
      "[Classifier Epoch 73/250] Loss: 0.4598\n",
      "[Classifier Epoch 74/250] Loss: 0.4584\n",
      "[Classifier Epoch 75/250] Loss: 0.4330\n",
      "[Classifier Epoch 76/250] Loss: 0.4653\n",
      "[Classifier Epoch 77/250] Loss: 0.4528\n",
      "[Classifier Epoch 78/250] Loss: 0.4597\n",
      "[Classifier Epoch 79/250] Loss: 0.4404\n",
      "[Classifier Epoch 80/250] Loss: 0.4374\n",
      "[Classifier Epoch 81/250] Loss: 0.4346\n",
      "[Classifier Epoch 82/250] Loss: 0.4588\n",
      "[Classifier Epoch 83/250] Loss: 0.4278\n",
      "[Classifier Epoch 84/250] Loss: 0.4497\n",
      "[Classifier Epoch 85/250] Loss: 0.4495\n",
      "[Classifier Epoch 86/250] Loss: 0.4498\n",
      "[Classifier Epoch 87/250] Loss: 0.4499\n",
      "[Classifier Epoch 88/250] Loss: 0.4420\n",
      "[Classifier Epoch 89/250] Loss: 0.4391\n",
      "[Classifier Epoch 90/250] Loss: 0.4258\n",
      "[Classifier Epoch 91/250] Loss: 0.4174\n",
      "[Classifier Epoch 92/250] Loss: 0.3904\n",
      "[Classifier Epoch 93/250] Loss: 0.4354\n",
      "[Classifier Epoch 94/250] Loss: 0.4263\n",
      "[Classifier Epoch 95/250] Loss: 0.4377\n",
      "[Classifier Epoch 96/250] Loss: 0.4332\n",
      "[Classifier Epoch 97/250] Loss: 0.4155\n",
      "[Classifier Epoch 98/250] Loss: 0.4126\n",
      "[Classifier Epoch 99/250] Loss: 0.3841\n",
      "[Classifier Epoch 100/250] Loss: 0.4381\n",
      "[Classifier Epoch 101/250] Loss: 0.4294\n",
      "[Classifier Epoch 102/250] Loss: 0.3980\n",
      "[Classifier Epoch 103/250] Loss: 0.3818\n",
      "[Classifier Epoch 104/250] Loss: 0.3896\n",
      "[Classifier Epoch 105/250] Loss: 0.4330\n",
      "[Classifier Epoch 106/250] Loss: 0.3784\n",
      "[Classifier Epoch 107/250] Loss: 0.4084\n",
      "[Classifier Epoch 108/250] Loss: 0.4275\n",
      "[Classifier Epoch 109/250] Loss: 0.3854\n",
      "[Classifier Epoch 110/250] Loss: 0.4073\n",
      "[Classifier Epoch 111/250] Loss: 0.4059\n",
      "[Classifier Epoch 112/250] Loss: 0.3948\n",
      "[Classifier Epoch 113/250] Loss: 0.3888\n",
      "[Classifier Epoch 114/250] Loss: 0.3944\n",
      "[Classifier Epoch 115/250] Loss: 0.3778\n",
      "[Classifier Epoch 116/250] Loss: 0.3749\n",
      "[Classifier Epoch 117/250] Loss: 0.3942\n",
      "[Classifier Epoch 118/250] Loss: 0.4020\n",
      "[Classifier Epoch 119/250] Loss: 0.4243\n",
      "[Classifier Epoch 120/250] Loss: 0.3818\n",
      "[Classifier Epoch 121/250] Loss: 0.3954\n",
      "[Classifier Epoch 122/250] Loss: 0.3776\n",
      "[Classifier Epoch 123/250] Loss: 0.3717\n",
      "[Classifier Epoch 124/250] Loss: 0.4034\n",
      "[Classifier Epoch 125/250] Loss: 0.3913\n",
      "[Classifier Epoch 126/250] Loss: 0.3880\n",
      "[Classifier Epoch 127/250] Loss: 0.3798\n",
      "[Classifier Epoch 128/250] Loss: 0.3991\n",
      "[Classifier Epoch 129/250] Loss: 0.3539\n",
      "[Classifier Epoch 130/250] Loss: 0.3971\n",
      "[Classifier Epoch 131/250] Loss: 0.3827\n",
      "[Classifier Epoch 132/250] Loss: 0.4015\n",
      "[Classifier Epoch 133/250] Loss: 0.3817\n",
      "[Classifier Epoch 134/250] Loss: 0.3757\n",
      "[Classifier Epoch 135/250] Loss: 0.3776\n",
      "[Classifier Epoch 136/250] Loss: 0.3834\n",
      "[Classifier Epoch 137/250] Loss: 0.3671\n",
      "[Classifier Epoch 138/250] Loss: 0.3798\n",
      "[Classifier Epoch 139/250] Loss: 0.3743\n",
      "[Classifier Epoch 140/250] Loss: 0.3557\n",
      "[Classifier Epoch 141/250] Loss: 0.3953\n",
      "[Classifier Epoch 142/250] Loss: 0.3711\n",
      "[Classifier Epoch 143/250] Loss: 0.3989\n",
      "[Classifier Epoch 144/250] Loss: 0.3797\n",
      "[Classifier Epoch 145/250] Loss: 0.3481\n",
      "[Classifier Epoch 146/250] Loss: 0.3586\n",
      "[Classifier Epoch 147/250] Loss: 0.3653\n",
      "[Classifier Epoch 148/250] Loss: 0.3375\n",
      "[Classifier Epoch 149/250] Loss: 0.3674\n",
      "[Classifier Epoch 150/250] Loss: 0.3767\n",
      "[Classifier Epoch 151/250] Loss: 0.3797\n",
      "[Classifier Epoch 152/250] Loss: 0.3923\n",
      "[Classifier Epoch 153/250] Loss: 0.4041\n",
      "[Classifier Epoch 154/250] Loss: 0.3794\n",
      "[Classifier Epoch 155/250] Loss: 0.3667\n",
      "[Classifier Epoch 156/250] Loss: 0.4052\n",
      "[Classifier Epoch 157/250] Loss: 0.3486\n",
      "[Classifier Epoch 158/250] Loss: 0.3675\n",
      "[Classifier Epoch 159/250] Loss: 0.3712\n",
      "[Classifier Epoch 160/250] Loss: 0.3672\n",
      "[Classifier Epoch 161/250] Loss: 0.3420\n",
      "[Classifier Epoch 162/250] Loss: 0.3606\n",
      "[Classifier Epoch 163/250] Loss: 0.3861\n",
      "[Classifier Epoch 164/250] Loss: 0.3547\n",
      "[Classifier Epoch 165/250] Loss: 0.3752\n",
      "[Classifier Epoch 166/250] Loss: 0.3422\n",
      "[Classifier Epoch 167/250] Loss: 0.3453\n",
      "[Classifier Epoch 168/250] Loss: 0.3507\n",
      "[Classifier Epoch 169/250] Loss: 0.3747\n",
      "[Classifier Epoch 170/250] Loss: 0.3591\n",
      "[Classifier Epoch 171/250] Loss: 0.3816\n",
      "[Classifier Epoch 172/250] Loss: 0.3696\n",
      "[Classifier Epoch 173/250] Loss: 0.3395\n",
      "[Classifier Epoch 174/250] Loss: 0.3543\n",
      "[Classifier Epoch 175/250] Loss: 0.3378\n",
      "[Classifier Epoch 176/250] Loss: 0.3878\n",
      "[Classifier Epoch 177/250] Loss: 0.3385\n",
      "[Classifier Epoch 178/250] Loss: 0.3467\n",
      "[Classifier Epoch 179/250] Loss: 0.3630\n",
      "[Classifier Epoch 180/250] Loss: 0.3740\n",
      "[Classifier Epoch 181/250] Loss: 0.3283\n",
      "[Classifier Epoch 182/250] Loss: 0.3598\n",
      "[Classifier Epoch 183/250] Loss: 0.3541\n",
      "[Classifier Epoch 184/250] Loss: 0.3548\n",
      "[Classifier Epoch 185/250] Loss: 0.3350\n",
      "[Classifier Epoch 186/250] Loss: 0.3662\n",
      "[Classifier Epoch 187/250] Loss: 0.3258\n",
      "[Classifier Epoch 188/250] Loss: 0.3459\n",
      "[Classifier Epoch 189/250] Loss: 0.3460\n",
      "[Classifier Epoch 190/250] Loss: 0.3415\n",
      "[Classifier Epoch 191/250] Loss: 0.3430\n",
      "[Classifier Epoch 192/250] Loss: 0.3342\n",
      "[Classifier Epoch 193/250] Loss: 0.3333\n",
      "[Classifier Epoch 194/250] Loss: 0.3204\n",
      "[Classifier Epoch 195/250] Loss: 0.3255\n",
      "[Classifier Epoch 196/250] Loss: 0.3748\n",
      "[Classifier Epoch 197/250] Loss: 0.3291\n",
      "[Classifier Epoch 198/250] Loss: 0.3580\n",
      "[Classifier Epoch 199/250] Loss: 0.3255\n",
      "[Classifier Epoch 200/250] Loss: 0.3508\n",
      "[Classifier Epoch 201/250] Loss: 0.3174\n",
      "[Classifier Epoch 202/250] Loss: 0.3743\n",
      "[Classifier Epoch 203/250] Loss: 0.3353\n",
      "[Classifier Epoch 204/250] Loss: 0.3647\n",
      "[Classifier Epoch 205/250] Loss: 0.3374\n",
      "[Classifier Epoch 206/250] Loss: 0.3743\n",
      "[Classifier Epoch 207/250] Loss: 0.3462\n",
      "[Classifier Epoch 208/250] Loss: 0.3518\n",
      "[Classifier Epoch 209/250] Loss: 0.3211\n",
      "[Classifier Epoch 210/250] Loss: 0.3143\n",
      "[Classifier Epoch 211/250] Loss: 0.3482\n",
      "[Classifier Epoch 212/250] Loss: 0.3276\n",
      "[Classifier Epoch 213/250] Loss: 0.3558\n",
      "[Classifier Epoch 214/250] Loss: 0.3133\n",
      "[Classifier Epoch 215/250] Loss: 0.3117\n",
      "[Classifier Epoch 216/250] Loss: 0.3196\n",
      "[Classifier Epoch 217/250] Loss: 0.3423\n",
      "[Classifier Epoch 218/250] Loss: 0.3482\n",
      "[Classifier Epoch 219/250] Loss: 0.3299\n",
      "[Classifier Epoch 220/250] Loss: 0.3068\n",
      "[Classifier Epoch 221/250] Loss: 0.3180\n",
      "[Classifier Epoch 222/250] Loss: 0.3001\n",
      "[Classifier Epoch 223/250] Loss: 0.3340\n",
      "[Classifier Epoch 224/250] Loss: 0.3633\n",
      "[Classifier Epoch 225/250] Loss: 0.3300\n",
      "[Classifier Epoch 226/250] Loss: 0.3155\n",
      "[Classifier Epoch 227/250] Loss: 0.3110\n",
      "[Classifier Epoch 228/250] Loss: 0.3308\n",
      "[Classifier Epoch 229/250] Loss: 0.3600\n",
      "[Classifier Epoch 230/250] Loss: 0.3325\n",
      "[Classifier Epoch 231/250] Loss: 0.3315\n",
      "[Classifier Epoch 232/250] Loss: 0.3203\n",
      "[Classifier Epoch 233/250] Loss: 0.3171\n",
      "[Classifier Epoch 234/250] Loss: 0.3105\n",
      "[Classifier Epoch 235/250] Loss: 0.3691\n",
      "[Classifier Epoch 236/250] Loss: 0.3261\n",
      "[Classifier Epoch 237/250] Loss: 0.3294\n",
      "[Classifier Epoch 238/250] Loss: 0.3271\n",
      "[Classifier Epoch 239/250] Loss: 0.3590\n",
      "[Classifier Epoch 240/250] Loss: 0.3376\n",
      "[Classifier Epoch 241/250] Loss: 0.2980\n",
      "[Classifier Epoch 242/250] Loss: 0.3338\n",
      "[Classifier Epoch 243/250] Loss: 0.3180\n",
      "[Classifier Epoch 244/250] Loss: 0.2627\n",
      "[Classifier Epoch 245/250] Loss: 0.3248\n",
      "[Classifier Epoch 246/250] Loss: 0.3461\n",
      "[Classifier Epoch 247/250] Loss: 0.3256\n",
      "[Classifier Epoch 248/250] Loss: 0.3018\n",
      "[Classifier Epoch 249/250] Loss: 0.2912\n",
      "[Classifier Epoch 250/250] Loss: 0.3141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Recs: 100%|██████████| 5148/5148 [06:16<00:00, 13.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b, eps=1e-9):\n",
    "    dot = np.dot(vec_a, vec_b)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    return dot / ((norm_a * norm_b) + eps)\n",
    "\n",
    "\n",
    "def load_and_merge_numeric_features(file_list, merge_on='id'):\n",
    "    base_df = pd.read_csv(file_list[0], sep='\\t')\n",
    "    for path in file_list[1:]:\n",
    "        df_next = pd.read_csv(path, sep='\\t')\n",
    "        base_df = pd.merge(base_df, df_next, on=merge_on, how='inner')\n",
    "    base_df.set_index(merge_on, inplace=True)\n",
    "    return base_df\n",
    "\n",
    "def normalize_features(df):\n",
    "    scaler = StandardScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
    "\n",
    "class Music4AllOnionDataset(Dataset):\n",
    "    def __init__(self, feature_df, label_df=None, transform=None):\n",
    "        self.transform = transform\n",
    "        feature_df = feature_df.sort_index()\n",
    "        self.feature_df = feature_df\n",
    "\n",
    "        if label_df is not None:\n",
    "            label_df = label_df.sort_index()\n",
    "            common_idx = feature_df.index.intersection(label_df.index)\n",
    "            self.feature_df = feature_df.loc[common_idx]\n",
    "            self.label_df = label_df.loc[common_idx]\n",
    "            self.has_labels = True\n",
    "        else:\n",
    "            self.label_df = None\n",
    "            self.has_labels = False\n",
    "\n",
    "        self.feature_data = self.feature_df.values.astype(np.float32)\n",
    "        if self.transform:\n",
    "            self.feature_data = self.transform(self.feature_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.feature_data[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        if self.has_labels:\n",
    "            y_val = self.label_df.iloc[idx].values\n",
    "            y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "            return x, y_val\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=128):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, reconstruct=False):\n",
    "        z = self.encoder(x)\n",
    "        if reconstruct:\n",
    "            return self.decoder(z)\n",
    "        return z\n",
    "\n",
    "class FineTuneClassifier(nn.Module):\n",
    "    def __init__(self, autoenc, latent_dim, num_classes):\n",
    "        super(FineTuneClassifier, self).__init__()\n",
    "        self.autoenc = autoenc\n",
    "        for param in self.autoenc.encoder.parameters():\n",
    "            param.requires_grad = False  \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            z = self.autoenc.encoder(x)  \n",
    "        logits = self.classifier(z)\n",
    "        return logits\n",
    "\n",
    "def train_autoenc(model, dataloader, num_epochs=50, lr=1e-3, patience=5, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon = model(x, reconstruct=True)\n",
    "            loss = criterion(x_recon, x)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "def train_classifier(\n",
    "    model, dataloader, num_epochs=20, lr=1e-3, patience=5, device='cuda'\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y.squeeze().long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"[Classifier Epoch {epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "def compute_all_embeddings(dataset, model, device='cuda'):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        item = dataset[i]\n",
    "        x = item[0] if isinstance(item, tuple) else item\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            z = model(x)\n",
    "        z_np = z.cpu().numpy().flatten()\n",
    "        all_embeddings.append(z_np)\n",
    "\n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def build_recommendation_matrix(embeddings, topK=10):\n",
    "    N = len(embeddings)\n",
    "    rec_matrix = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "    for i in tqdm(range(N), desc=\"Building Recs\"):\n",
    "        vec_i = embeddings[i]\n",
    "        sims = [cosine_similarity(vec_i, embeddings[j]) for j in range(N)]\n",
    "        sims = np.array(sims)\n",
    "        sims[i] = -1e9\n",
    "        topk_idx = sims.argsort()[::-1][:topK]\n",
    "        topk_vals = sims[topk_idx]\n",
    "        rec_matrix[i, topk_idx] = topk_vals\n",
    "\n",
    "    return rec_matrix\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "numeric_files = [\n",
    "        \"./dataset/id_blf_correlation_mmsr.tsv\",\n",
    "        \"./dataset/id_blf_deltaspectral_mmsr.tsv\",\n",
    "        \"./dataset/id_blf_logfluc_mmsr.tsv\",\n",
    "        \"./dataset/id_blf_spectral_mmsr.tsv\",\n",
    "        \"./dataset/id_blf_spectralcontrast_mmsr.tsv\",\n",
    "        \"./dataset/id_blf_vardeltaspectral_mmsr.tsv\",\n",
    "        \"./dataset/id_incp_mmsr.tsv\",\n",
    "        \"./dataset/id_ivec256_mmsr.tsv\",\n",
    "        \"./dataset/id_ivec512_mmsr.tsv\",\n",
    "        \"./dataset/id_ivec1024_mmsr.tsv\",\n",
    "        \"./dataset/id_lyrics_tf-idf_mmsr.tsv\",\n",
    "        \"./dataset/id_lyrics_word2vec_mmsr.tsv\",\n",
    "        \"./dataset/id_lyrics_bert_mmsr.tsv\",\n",
    "        \"./dataset/id_mfcc_bow_mmsr.tsv\",\n",
    "        \"./dataset/id_mfcc_stats_mmsr.tsv\",\n",
    "        \"./dataset/id_musicnn_mmsr.tsv\",\n",
    "        \"./dataset/id_resnet_mmsr.tsv\",\n",
    "        \"./dataset/id_vgg19_mmsr.tsv\",\n",
    "        \"./dataset/id_total_listens.tsv\"\n",
    "]\n",
    "\n",
    "fused_df = load_and_merge_numeric_features(numeric_files, merge_on='id')\n",
    "fused_df_normalized = normalize_features(fused_df)\n",
    "\n",
    "labels_path = \"./dataset/id_genres_mmsr.tsv\"\n",
    "if os.path.exists(labels_path):\n",
    "        labels_df = pd.read_csv(labels_path, sep='\\t').set_index('id')\n",
    "        unique_genres = labels_df['genre'].unique()\n",
    "        genre_to_idx = {g: i for i, g in enumerate(unique_genres)}\n",
    "        labels_df['genre_id'] = labels_df['genre'].map(genre_to_idx)\n",
    "        final_labels_df = labels_df[['genre_id']]\n",
    "else:\n",
    "    final_labels_df = None\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(\n",
    "        fused_df_normalized, final_labels_df, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Music4AllOnionDataset(feature_train, label_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "test_dataset = Music4AllOnionDataset(feature_test, label_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "input_dim = fused_df_normalized.shape[1]\n",
    "latent_dim = 128\n",
    "\n",
    "autoenc_model = AutoEncoder(input_dim, latent_dim=latent_dim)\n",
    "train_autoenc(autoenc_model, train_loader, num_epochs=250, lr=1e-3, patience=100, device=device)\n",
    "\n",
    "if final_labels_df is not None:\n",
    "    num_classes = len(np.unique(final_labels_df['genre_id']))\n",
    "    classifier_model = FineTuneClassifier(autoenc_model, latent_dim, num_classes)\n",
    "    train_classifier(classifier_model, train_loader, num_epochs=250, patience=100, lr=1e-3, device=device)\n",
    "\n",
    "full_dataset = Music4AllOnionDataset(fused_df_normalized)\n",
    "embeddings = compute_all_embeddings(full_dataset, autoenc_model, device=device)\n",
    "rec_matrix = build_recommendation_matrix(embeddings, topK=100)\n",
    "np.savetxt(\"./predictions/rets_auto_enc_100_matrix\", rec_matrix, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
